{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score, mean_absolute_error,mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font',size=6)\n",
    "SEED=1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data\\\\train.csv\"\n",
    "test_file = \"data\\\\test.csv\"\n",
    "sub_file = \"data\\\\sub.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = pd.read_csv(data_file)\n",
    "display(housing_data.head())\n",
    "display(housing_data.describe())\n",
    "\n",
    "y=housing_data[\"SalePrice\"]\n",
    "#housing_data.drop(\"SalePrice\")\n",
    "features = housing_data.columns\n",
    "num_features = len(features)-2\n",
    "#housing_data = housing_data.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\n",
    "\n",
    "\n",
    "print (f\"Num features in dataset: {num_features}\")\n",
    "print(features)\n",
    "cats = ['MSZoning', 'Street','Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
    "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "       'HouseStyle', 'OverallQual', 'OverallCond', \n",
    "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
    "        'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "       'BsmtFinType2', 'Heating',\n",
    "       'HeatingQC', 'CentralAir', 'Electrical',  'KitchenQual',\n",
    "        'Functional', 'FireplaceQu', 'GarageType',\n",
    "       'GarageFinish', 'GarageQual',\n",
    "       'GarageCond', 'PavedDrive',  'PoolQC',\n",
    "       'Fence', 'MiscFeature', 'SaleType',\n",
    "       'SaleCondition']\n",
    "\n",
    "non_cats= [c for c in features if c not in cats]\n",
    "print(non_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=11\n",
    "cols=8\n",
    "fig,ax = plt.subplots(rows,cols,figsize=(30, 32))\n",
    "i=0\n",
    "j=0\n",
    "for f in features:\n",
    "    print(f\"Plotting feature {f}\")\n",
    "   \n",
    "    # if f==\"SalePrice\":\n",
    "    #     continue \n",
    "    # if f in cats:\n",
    "    #     housing_data=housing_data.astype({f:'category'})\n",
    "    \n",
    "    x=housing_data[f]\n",
    "    ax[i,j].scatter(x[x.notna()],y[x.notna()])\n",
    "    ax[i,j].set_title(f)\n",
    "    j=j+1\n",
    "    if j==cols:\n",
    "        j=0\n",
    "        i=i+1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=housing_data.apply(lambda x : pd.factorize(x)[0]).corr()\n",
    "mask = (abs(corr) > 0.5) & (abs(corr) != 1)\n",
    "corr.where(mask).stack().sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=3)\n",
    "pca_2\n",
    "pca_2.fit(housing_data.apply(lambda x : pd.factorize(x)[0]))\n",
    "pca_2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans = pca_2.transform(housing_data.apply(lambda x : pd.factorize(x)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection ='3d')\n",
    "\n",
    "ax.scatter(housing_data[\"LotArea\"],housing_data[\"LotFrontage\"],y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "# One hot encoding\n",
    "display(housing_data.head())\n",
    "\n",
    "\n",
    "onehot_data = pd.get_dummies(data = housing_data,\n",
    "                        prefix = cats,\n",
    "                        columns = cats)\n",
    "onehot_data= onehot_data.drop(columns=[\"SalePrice\"],axis=1)\n",
    "for f in non_cats:\n",
    "    c = housing_data[f].corr(y)\n",
    "    CORRPAR=\"~~WEAK~~\"\n",
    "    if abs(c) >0.5:\n",
    "        CORRPAR = \"++HIGH++\"\n",
    "    elif abs(c)<0.2:\n",
    "        CORRPAR = \"--LOW--\"\n",
    "        print (f\"Dropping Feature {f}\")\n",
    "        onehot_data.drop(f,axis=1)\n",
    "        \n",
    "    print (f\"Correlation between {f} and Sale price is {c} - {CORRPAR}\")\n",
    "# Initialize the class\n",
    "#print(onehot_data[\"SalePrice\"])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Linear Regression Gradient Descent model\n",
    "def run_reg(X_train,y_train,X_cv,y_cv,plot):\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    # Train the model\n",
    "    linear_model.fit(X_train, y_train )\n",
    "    yhat_train = linear_model.predict(X_train) \n",
    "    train_mse = mean_squared_error(y_train, yhat_train) / 2\n",
    "    yhat_cv = linear_model.predict(X_cv)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat_cv) / 2\n",
    "    print(f\"Training Error: {train_mse} Validation Error {cv_mse}\")\n",
    "    \n",
    "    if (plot):\n",
    "        plt.scatter(X_train[:,0],y_train)\n",
    "        plt.scatter(X_train[:,0],yhat_train,color=\"r\")\n",
    "        plt.show()\n",
    "        plt.scatter(X_cv[:,0],y_cv)\n",
    "        plt.scatter(X_cv[:,0],yhat_cv,color=\"r\")\n",
    "        plt.show()\n",
    "    return train_mse,cv_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_cv_err_list=[]\n",
    "onehot_data = onehot_data.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\n",
    "print(f\"Total data size: {onehot_data.shape[0]}\")\n",
    "print(onehot_data.columns)\n",
    "pre_drop_f = [\"2ndFlrSF\",\"1stFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\"] #Since Dropping these reduces error significantly\n",
    "dropped_features=[]\n",
    "for f in features:\n",
    "    df=onehot_data\n",
    "    df=df.drop(pre_drop_f,axis=1)\n",
    "    if f in df.columns:\n",
    "        print(f\"Dropping feature {f}\")\n",
    "        df=df.drop(f,axis=1)\n",
    "        dropped_features.append(f)\n",
    "    else: \n",
    "        continue\n",
    "\n",
    "    X_train,X_cv,y_train,y_cv=train_test_split(df,y,train_size=0.70,random_state = SEED)\n",
    "\n",
    "\n",
    "    y_train=np.asarray(y_train)\n",
    "    X_train=np.asarray(X_train)\n",
    "    y_cv=np.asarray(y_cv)\n",
    "    X_cv=np.asarray(X_cv)\n",
    "    scaler_linear = StandardScaler()\n",
    "    # Compute the mean and standard deviation of the training set then transform it\n",
    "    X_train_scaled=scaler_linear.fit_transform(X_train)\n",
    "    X_cv_scaled=scaler_linear.transform(X_cv)\n",
    "    #print(scaled_housing_data)    \n",
    "    print(f\"Training data size: {X_train_scaled.shape}\")\n",
    "    print(f\"Validtion data size: {X_cv_scaled.shape}\")\n",
    "\n",
    "    print(f\"Gradient Descent - Linear Regression\")\n",
    "    print(\"=\"*50)\n",
    "    mse_err,cv_err = run_reg(X_train,y_train,X_cv,y_cv,plot=False)\n",
    "    fd_cv_err_list.append(cv_err)\n",
    "\n",
    "min_i = np.argmin(fd_cv_err_list)\n",
    "print(f\"Dropping feature {dropped_features[i]} results  min Validation error {fd_cv_err_list[min_i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Features\n",
    "print(f\"Total data size: {onehot_data.shape[0]}\")\n",
    "print(onehot_data.columns)\n",
    "onehot_data = onehot_data.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\n",
    "\n",
    "## Nothing reduces error significantly here. Trying options based on intuition and Linear Regression results\n",
    "#pre_drop_f = [\"Id\",\"2ndFlrSF\",\"1stFlrSF\",\"LowQualFinSF\"]#,\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\"] #\n",
    "#for f in features:\n",
    "df=onehot_data\n",
    "#    df=df.drop(pre_drop_f,axis=1)\n",
    "# if f in df.columns:\n",
    "#     print(f\"Dropping feature {f}\")\n",
    "#     df=df.drop(f,axis=1)\n",
    "#     dropped_features.append(f)\n",
    "# else: \n",
    "#     continue\n",
    "\n",
    "X_train,X_cv,y_train,y_cv=train_test_split(df,y,train_size=0.70,random_state = SEED)\n",
    "y_train=np.asarray(y_train)\n",
    "X_train=np.asarray(X_train)\n",
    "y_cv=np.asarray(y_cv)\n",
    "X_cv=np.asarray(X_cv)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Compute the number of features and transform the training set\n",
    "X_train_mapped = poly.fit_transform(X_train)\n",
    "\n",
    "scaler_poly = StandardScaler()\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "\n",
    "X_cv_mapped = poly.transform(X_cv)\n",
    "# Scale the cross validation set using the mean and standard deviation of the training set\n",
    "X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "print(f\"Gradient Descent - Second order Poly Regression\")\n",
    "print(\"=\"*50)\n",
    "mse_err,cv_err = run_reg(X_train_mapped_scaled,y_train,X_cv_mapped_scaled,y_cv,plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to save the errors, models, and feature transforms\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "X_train,X_cv,y_train,y_cv=train_test_split(df,y,train_size=0.70,random_state = SEED)\n",
    "# y_train=np.asarray(y_train)\n",
    "# X_train=np.asarray(X_train)\n",
    "# y_cv=np.asarray(y_cv)\n",
    "# X_cv=np.asarray(X_cv)\n",
    "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "for degree in range(1,3):#11:  - Unfortunately runs out of memory on my computer for 3rd degree poly\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(X_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    X_cv_mapped = poly.transform(X_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "    # Create and train the model\n",
    "    print(f\"Gradient Descent - {degree} order Poly Regression\")\n",
    "\n",
    "    train_mse,cv_mse = run_reg(X_train_mapped_scaled,y_train,X_cv_mapped_scaled,y_cv,plot=True)\n",
    "    train_mses.append(train_mse)\n",
    "    cv_mses.append(cv_mse)\n",
    "    \n",
    "# Plot the results\n",
    "degrees=range(1,11)\n",
    "#plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "#plt.scatter(degrees, train_mses)\n",
    "#plt.scatter(degrees, cv_mses)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(poly,scaler_poly,model):\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    display(test_data.head())\n",
    "    \n",
    "    display(test_data.describe())\n",
    "\n",
    "    test_onehot_data = pd.get_dummies(data = test_data,\n",
    "                        prefix = cats,\n",
    "                        columns = cats)\n",
    "    \n",
    "    drop_fs= [c for c in test_onehot_data.columns if c not in onehot_data.columns and c in test_onehot_data.columns]\n",
    "    print(f\"Columns to drop = {drop_fs}\")\n",
    "    test_onehot_data=test_onehot_data.drop(columns=drop_fs,axis=1)\n",
    "    for c in onehot_data.columns:\n",
    "        if c not in test_onehot_data:\n",
    "            if onehot_data[c].dtype.kind in 'biufc':\n",
    "                test_onehot_data.insert(onehot_data.columns.get_loc(c), c,0)\n",
    "            else:\n",
    "                test_onehot_data.insert(onehot_data.columns.get_loc(c), c,'.')\n",
    "    print(f\"Train Data Columns: {onehot_data.columns}\")\n",
    "    print(f\"Test Data Columns: {test_onehot_data.columns}\")\n",
    "    test_onehot_data = test_onehot_data.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\n",
    "    display(test_data.describe())\n",
    "\n",
    "    #test_onehot_data=test_onehot_data.drop(columns=[\"Id\"],axis=1)\n",
    "    #test_onehot_data=np.asarray(test_onehot_data)\n",
    "    #print(test_onehot_data.shape)\n",
    "    X_test_mapped = poly.transform(test_onehot_data)\n",
    "    X_test_mapped_scaled = scaler_poly.transform(X_test_mapped)  \n",
    "    y_pred = model.predict(X_test_mapped_scaled) \n",
    "    print(y_pred.shape)\n",
    "    display(test_data.describe())\n",
    "\n",
    "    test_data.insert(len(test_data.columns),\"SalePrice\",y_pred)\n",
    "    test_data.to_csv(sub_file,columns=[\"Id\",\"SalePrice\"],index=False)\n",
    "    return (y_pred,test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(X_train)\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "# Scale the cross validation set using the mean and standard deviation of the training set\n",
    "model = LinearRegression()\n",
    "# Train the model\n",
    "model.fit(X_train_mapped_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred,t = run_test(poly,scaler_poly,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model\n",
    "tf.random.set_seed(SEED)  # applied to achieve consistent results\n",
    "scaler_linear = StandardScaler()\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "X_train_scaled=scaler_linear.fit_transform(X_train)\n",
    "X_cv_scaled=scaler_linear.transform(X_cv)\n",
    "n=X_train.shape[1]\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(n,)),\n",
    "        Dense(256, activation=\"relu\", name=\"layer1\"),#kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)),\n",
    "        Dense(128, activation=\"relu\", name=\"layer2\"),#kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)),\n",
    "        Dense(64, activation=\"relu\", name=\"layer3\"),#kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)),\n",
    "        Dense(32, activation=\"relu\", name=\"layer4\"),#kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)),\n",
    "\n",
    "        Dense(1,name=\"layer5\"),\n",
    "    ]\n",
    ")\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W5, b5 = model.get_layer(\"layer5\").get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W5.shape}:\\n\", W5, f\"\\nb2{b5.shape}:\", b5)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning with LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m     \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m     23\u001b[0m yhat_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_scaled) \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menumerate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:689\u001b[0m, in \u001b[0;36mTFEpochIterator.enumerate_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 689\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches:\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m    692\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution\n\u001b[0;32m    693\u001b[0m         ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Linear Regression NN\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models=[]\n",
    "learning_rates =[0.01,0.001,0.0001,0.00001,0.000001]\n",
    "epochs = [300,300, 3000,30000,300000]\n",
    "X_train,X_cv,y_train,y_cv=train_test_split(onehot_data,y,train_size=0.70,random_state = SEED)\n",
    "for lr,ep in zip(learning_rates,epochs):\n",
    "     print(f\"X_train Shape {X_train.shape} Y_train Shape {y_train.shape}\")\n",
    "     print(f\"Running with LR: {lr} and {ep} epochs\")\n",
    "\n",
    "\n",
    "     model.compile(\n",
    "     loss=\"mse\",\n",
    "     optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "     )\n",
    "     model.fit(\n",
    "          X_train_scaled, y_train,\n",
    "          epochs=ep,\n",
    "          verbose=1\n",
    "     )\n",
    "     models.append(model)\n",
    "     yhat_train = model.predict(X_train_scaled) \n",
    "     train_mse = mean_squared_error(y_train, yhat_train) / 2\n",
    "     yhat_cv = model.predict(X_cv_scaled)\n",
    "     cv_mse = mean_squared_error(y_cv, yhat_cv) / 2\n",
    "     train_mses.append(train_mse)\n",
    "     cv_mses.append(cv_mse)\n",
    "     print(f\"X_train Shape {X_train.shape} Yhat_train Shape {yhat_train.shape}, features: {n}\")\n",
    "     print(f\"Training Error: {train_mse} Validation Error {cv_mse}, features: {n}\")\n",
    "\n",
    "     plt.scatter(X_train_scaled[:,0],y_train)\n",
    "     plt.scatter(X_train_scaled[:,0],yhat_train,color=\"r\")\n",
    "     plt.show()\n",
    "best_i = np.argmin(cv_mse)\n",
    "best_model = models[i]\n",
    "print(f\"Identified best LR as {learning_rates[i]} with error {cv_mses[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_file)\n",
    "test_onehot_data = pd.get_dummies(data = test_data,\n",
    "                    prefix = cats,\n",
    "                    columns = cats)\n",
    "\n",
    "drop_fs= [c for c in test_onehot_data.columns if c not in onehot_data.columns and c in test_onehot_data.columns]\n",
    "test_onehot_data=test_onehot_data.drop(columns=drop_fs,axis=1)\n",
    "for c in onehot_data.columns:\n",
    "    if c not in test_onehot_data:\n",
    "        if onehot_data[c].dtype.kind in 'biufc':\n",
    "            test_onehot_data.insert(onehot_data.columns.get_loc(c), c,0)\n",
    "        else:\n",
    "            test_onehot_data.insert(onehot_data.columns.get_loc(c), c,'.')\n",
    "test_onehot_data = test_onehot_data.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\n",
    "\n",
    "X_test_scaled = scaler_linear.transform(test_onehot_data)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "test_data.insert(len(test_data.columns),\"SalePrice\",y_pred)\n",
    "test_data.to_csv(sub_file,columns=[\"Id\",\"SalePrice\"],index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
